# ğŸ“„ Local LLM RAG PDF QA Tool

A **lightweight, offline-ready Retrieval-Augmented Generation (RAG)** pipeline to query PDF documents using **local embeddings** and a **local language model**. Built with `LangChain`, `FAISS`, `sentence-transformers`, and `transformers`, this project enables **privacy-conscious document Q&A** directly from the command line â€” no cloud APIs required.

Question asked: "How do we answer queries from local PDFs without hallucination?"
The way I answered this: Retrieval-Augmented Generation (RAG) enhances LLM performance by grounding generation in real-world documents, improving factual accuracy and reducing hallucinations. This prototype demonstrates a simple local RAG pipeline for PDF document QA.
---


## ğŸš€ Features

- ğŸ” Ask **natural language questions** about any PDF (e.g., company policies, academic papers)
- ğŸ§  Uses **local models only** â€” no OpenAI or internet-based APIs
- âš™ï¸ Command-line interface for quick, secure use
- ğŸ“¦ Production-grade structure with modular utilities and CI/CD readiness
- ğŸ§  Based on LangChain + FAISS + HuggingFace Transformers

---

## Technical Stack

* Python
* FAISS for vector search
* Hugging Face Transformers
* SentenceTransformers for embedding model

## ğŸ“ Project Structure

    llm_rag/
    â”œâ”€â”€ ingest.py             # Downloads, splits, and embeds PDF
    â”œâ”€â”€ query.py              # CLI interface for asking questions
    â”œâ”€â”€ utils.py              # Shared helpers for embedding, querying, loading
    â”œâ”€â”€ docs/                 # Folder to hold input PDFs
    â”œâ”€â”€ faiss_index_local/    # Saved vector index (auto-created)
    â”œâ”€â”€ requirements.txt      # Install dependencies
    â””â”€â”€ README.md             # Project documentation

---

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/your-username/local-llm-rag-pdf-qa.git
cd local-llm-rag-pdf-qa

# (Recommended) Create a virtual environment
python -m venv rag_env
source rag_env/bin/activate  # or .\rag_env\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt

## ğŸ§© Models Used

| Component      | Model                                   |
|----------------|------------------------------------------|
| Embeddings     | `sentence-transformers/all-MiniLM-L6-v2` |
| LLM (CPU safe) | `google/flan-t5-base`                    |


You can swap these models out for larger or GPU-based models if needed.

---

## ğŸ“¥ How to Ingest a PDF

This script will:
1. Download a sample PDF (if missing)
2. Chunk the text
3. Embed and store it in FAISS

```bash
python ingest.py

---

## â“ How to Ask Questions (CLI)

Run a natural language query from the terminal:

    python query.py "What is Apple's policy on conflicts of interest?"

The system will return:
- âœ… A generated answer  
- ğŸ“„ The source document metadata

---

## ğŸ§ª Example Output

    âœ… Answer:
    Employees must avoid situations where their personal interests conflict with the interests of Apple...

    ğŸ“„ Sources:
     - docs/company_policy.pdf

---

## ğŸ§° Extending the Project

- ğŸ”„ Swap in a GPU LLM like `mistralai/Mistral-7B-Instruct`
- ğŸŒ Add a Streamlit or Gradio UI for web access
- ğŸ“š Embed multiple PDFs for multi-document retrieval
- âœ… Add `pre-commit`, linting, and GitHub Actions for CI/CD
- ğŸ§ª Add test suite for core utilities

---

## ğŸ›¡ï¸ License

This project is open-source under the [MIT License](LICENSE).

---

## ğŸ™‹â€â™€ï¸ Maintainer

**Sinjini Mitra**  
ğŸ“ PhD in AI/ML | ğŸ§  Builder of interpretable, practical ML tools  
[LinkedIn](https://www.linkedin.com/in/sinjini-mitra/) 